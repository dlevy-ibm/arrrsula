---
- name: verify disks have been provided
  fail: msg="please provide osd disks"
  when: ceph.disks is not defined

- name: verify sane loopback settings
  fail: msg="Using loopback devices with ceph requires one and only one device"
  when: ceph.use_loopback and ceph.disks|length != 1

- name: install dependencies
  apt: pkg={{ item }}
       state=present
  with_items: ceph.osd_pkgs

- name: register ceph.keyring
  slurp: src=/var/lib/ceph/bootstrap-osd/ceph.keyring
  register: ceph_keyrings
  run_once: true
  delegate_to: "{{ groups['ceph_monitors'][0] }}"

- name: write ceph.keyring
  copy:
    dest: "{{ ceph_keyrings['source'] }}"
    content: "{{ ceph_keyrings['content'] | b64decode }}"

- name: create loopback file
  when: ceph.use_loopback
  command: /bin/dd if=/dev/zero of={{ ceph.loopback.file }} bs=1024 count={{ ceph.loopback.size }}
  args:
    creates: "{{ ceph.loopback.file }}"

- name: see if loopback file exists
  shell: losetup --show /dev/loop0
  register: loopback_exists
  ignore_errors: true

- name: create loopback device
  when: ceph.use_loopback and loopback_exists.rc != 0
  command: /sbin/losetup /dev/loop0 {{ ceph.loopback.file }}

- name: check if device has a partition table
  when: ceph.use_loopback
  shell: "parted --script /dev/loop0 print | grep 'Partition Table: unknown'"
  register: has_partition_table
  ignore_errors: true

- name: create partition table
  when: ceph.use_loopback and has_partition_table.rc == 0
  shell: "parted --script /dev/loop0 mklabel msdos"

- name: check if 'ceph' partition exists on osd disks
  shell: "parted --script /dev/{{ item }} print | egrep -sq 'ceph'"
  with_items: ceph.disks
  ignore_errors: true
  changed_when: false
  register: ceph_partitions

# journal_collocation
- name: prepare osd disks
  command: ceph-disk prepare /dev/{{ item.1 }}
  with_together:
    - ceph_partitions.results
    - ceph.disks
  when: item.0.rc != 0

- name: check if ceph disk is activated
  command: '[ "$(ls -A /var/lib/ceph/osd)" ] && echo "not empty" || echo "empty"'
  register: osd_ls
  ignore_errors: true

- name: activate osds
  command: ceph-disk activate /dev/{{ item.1 }}
  with_together:
    - ceph_partitions.results
    - ceph.disks
  when: osd_ls.stdout == "empty"

- name: start and add that the osd service(s) to the init sequence
  service: name=ceph
           state=started
           enabled=yes

- name: delete default 'rbd' pool
  command: ceph osd pool delete rbd rbd --yes-i-really-really-mean-it
  register: poolout
  changed_when: poolout.stdout | search('removed')
  run_once: true
  delegate_to: "{{ groups['ceph_monitors'][0] }}"

- name: create openstack pool
  ceph_pool:
    pool_name: "{{ ceph.pool_name }}"
  register: pool_output
  run_once: true
  delegate_to: "{{ groups['ceph_monitors'][0] }}"

- include: system_tuning.yml
  tags: ceph-osd
